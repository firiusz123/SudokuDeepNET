{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e51184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lr, num_classes, load_path=None, freeze_features=True):\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    if freeze_features:\n",
    "        # Freeze all layers except the classifier\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    if load_path:\n",
    "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "        model.eval()\n",
    "    optimizer  = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    return model, optimizer, nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl,which_epoch_unfreeze, writer):\n",
    "    model.to(device)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(opt, step_size=1, gamma=0.2)  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if epoch  == which_epoch_unfreeze :\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = True\n",
    "            #params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "            opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for xb, yb in progress_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            loss, batch_size = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            total_loss += loss * batch_size\n",
    "            count += batch_size\n",
    "            progress_bar.set_postfix(train_loss=total_loss / count)\n",
    "            train_loss = total_loss / count\n",
    "            writer.add_scalar('Loss/train', train_loss, count)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_nums = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                loss, batch_size = loss_batch(model, loss_func, xb, yb)\n",
    "                val_losses.append(loss)\n",
    "                val_nums.append(batch_size)\n",
    "\n",
    "        val_loss = np.sum(np.multiply(val_losses, val_nums)) / np.sum(val_nums)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Final Validation Loss = {val_loss:.4f}\")  \n",
    "\n",
    "def test_model(model, test_loader, loss_func, class_names=None , writer = None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = loss_func(outputs, yb)\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == yb).sum().item()\n",
    "            total_samples += xb.size(0)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names if class_names else range(cm.shape[1]),\n",
    "                yticklabels=class_names if class_names else range(cm.shape[0]))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix\\nLoss: {avg_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if writer:\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        image = Image.open(buf)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        writer.add_image(\"Confusion_Matrix\", image ,1)\n",
    "    plt.show()\n",
    "\n",
    "    return avg_loss, accuracy, cm\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
